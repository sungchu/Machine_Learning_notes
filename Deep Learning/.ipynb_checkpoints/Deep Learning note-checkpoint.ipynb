{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning 學習筆記\n",
    "本筆記是基於「Deep Learning - 用 Python 進行深度學習的基礎理論實作」所撰寫的筆記"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 感知器\n",
    "- 感知器是收到多個輸入訊號之後，在當作一個訊號輸出\n",
    "- 邏輯電路\n",
    "   - 單層感知器的邏輯電路: 表現線性區域\n",
    "      - 及閘(AND Gate)\n",
    "      - 反及閘(NAND Gate)\n",
    "      - 或閘(OR Gate)\n",
    "   - 多層感知器的邏輯電路: 表現非線性區域\n",
    "      - 互斥或閘(XOR Gate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活化函數\n",
    "- 以臨界值為分界來轉換輸出的函數\n",
    "- 決定如何活化輸入的訊號總和(如何發火)的功能\n",
    "- 常見的有階梯函數、sigmoid 函數及 ReLU 函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 階梯函數\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 函數\n",
    "- <font size = 4>$ h(x)=\\frac{1} {1+exp(-x)} $ </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "x = np.arange(-5.0,5.0,0.1)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x,y)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU 函數\n",
    "- sigmoid 函數在神經網路的歷史中，從很早之前就開始使用，但最近比較常用的是 ReLU 函數\n",
    "- 輸入超過0，就會將輸入直接輸出，如果是0以下，就輸出0的函數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def relu(x):\n",
    "    return np.maximum(0,x)\n",
    "x = np.arange(-5.0,5.0,0.1)\n",
    "y = relu(x)\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 輸出層\n",
    "- 回歸問題要使用恆等函數\n",
    "- 兩個分類的類別問題使用 sigmoid 函數\n",
    "- 多分類的類別問題使用 softmax 函數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恆等函數: 把輸入值直接輸出\n",
    "```python\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identityfunction(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### softmax 函數\n",
    "- <font size = 4>$ y_k=\\frac{exp(a_k)} {\\sum_{i=1}^{n} exp(a_i)} $ </font>\n",
    "   - $ y_k $ 是計算第 k 個輸出\n",
    "   - $ a_k $ 是輸入的第 k 個訊號\n",
    "- 輸出的是 0 到 1.0 之間的實數，且輸出的總和為 1，使得 softmax 函數的輸出可以解釋為機率\n",
    "- 一般而言，神經網路的類別會把相當於輸出最大神經元的類別當作辨識結果。即使套用 softmax 函數，輸出最大神經元的位置不變。神經網路進行分類時，一般會省略輸出層的 softmax 函數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    exp_a = np.exp(a)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由於指數的值可能會變很大，而造成溢位，因此有了下方的改良方法\n",
    "\n",
    "\n",
    "<font size = 4>\n",
    "$ y_k=\\frac{exp(a_k)} {\\sum_{i=1}^{n} exp(a_i)} $ \n",
    "= $ y_k=\\frac{C exp(a_k)} {C\\sum_{i=1}^{n} exp(a_i)} $ \n",
    "= $ y_k=\\frac{C exp(a_k+logC)} {C\\sum_{i=1}^{n} exp(a_i+logC)} $\n",
    "= $ y_k=\\frac{C exp(a_k+C')} {C\\sum_{i=1}^{n} exp(a_i+C')} $\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    c = np.max(a) # 為了防範溢位，一般會使用輸入訊號中，最大的數值\n",
    "    exp_a = np.exp(a - c)\n",
    "    sum_exp_a = np.sum(exp_a)\n",
    "    y = exp_a / sum_exp_a\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST 資料集: 手寫數字影像集\n",
    "- load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "   - load_mnist的函數是以「(訓練影像，訓練標籤)，(測試影像，測試標籤)」\n",
    "   - normalize 是設定輸入影像是否正規化為0.0~1.0的值\n",
    "      - 如果為 False，輸入影像的像素就維持原本的0~255\n",
    "   - flatten 是設定輸入影像是否變平(一維陣列)\n",
    "      - 如果為 False，輸入影像就是1*28*28的三維陣列\n",
    "      - 如果是 True，將儲存由784個元素形成的一維陣列\n",
    "   - one_hot_label 是設定是否儲存成 one-hot 編碼。one-hot編碼是指，如[0,0,1,0,0,0,0,0,0,0]，這種只有正確答案的標籤為1，其餘為0的陣列\n",
    "      - 如果是 False，就會儲存 7,2 這種單純成為正確答案的標籤\n",
    "      - 如果為 True，就儲存成 one-hot 編碼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "#sys.path.append(os.pardir) # 載入父目錄檔案的設定\n",
    "from mnist import load_mnist\n",
    "\n",
    "# 一開始呼叫該檔案\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "# 分別輸出資料的形狀\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)\n",
    "print(x_test.shape)\n",
    "print(t_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "(784,)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from mnist import load_mnist\n",
    "from PIL import Image\n",
    "\n",
    "def img_show(img):\n",
    "    pil_img = Image.fromarray(np.uint8(img))\n",
    "    pil_img.show()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)\n",
    "\n",
    "img = x_train[0]\n",
    "label = t_train[0]\n",
    "print(label)\n",
    "\n",
    "print(img.shape)\n",
    "img = img.reshape(28,28) # 將形狀變形成原本的影像大小\n",
    "print(img.shape) \n",
    "\n",
    "img_show(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試資料\n",
    "## 輸入層的 784(28*28) 個神經元，輸出層的 10 個神經元\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.pardir) \n",
    "import numpy as np\n",
    "import pickle\n",
    "from mnist import load_mnist\n",
    "from functions import sigmoid, softmax\n",
    "\n",
    "\n",
    "def get_data():\n",
    "    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)\n",
    "    return x_test, t_test\n",
    "\n",
    "# 在 init_network()中，載入儲存 pickle 檔案 sample_weight.pkl 中，已經學習完畢的權重參數。\n",
    "# 在這個檔案中，權重與偏權值的參數會儲存成字典型態的變數。\n",
    "def init_network():\n",
    "    with open(\"sample_weight.pkl\", 'rb') as f: # .pkl是 pickle 檔案，此功能可以把值形程式的物件，當作檔案儲存起來\n",
    "        network = pickle.load(f)\n",
    "    return network\n",
    "\n",
    "\n",
    "def predict(network, x):\n",
    "    W1, W2, W3 = network['W1'], network['W2'], network['W3']\n",
    "    b1, b2, b3 = network['b1'], network['b2'], network['b3']\n",
    "\n",
    "    a1 = np.dot(x, W1) + b1\n",
    "    z1 = sigmoid(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    z2 = sigmoid(a2)\n",
    "    a3 = np.dot(z2, W3) + b3\n",
    "    y = softmax(a3)\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(len(x)):\n",
    "    y = predict(network, x[i])\n",
    "    p = np.argmax(y) # 取得陣列中最大值的索引值\n",
    "    if p == t[i]: # 神經網路預測的答案與正確答案標籤比對\n",
    "        accuracy_cnt += 1\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.9352\n"
     ]
    }
   ],
   "source": [
    "# 把上個 cell 的內容改成批次處理的寫法，批次處理可以縮短影像處理的時間\n",
    "x, t = get_data()\n",
    "network = init_network()\n",
    "\n",
    "batch_size = 100 # 批次處理的量\n",
    "accuracy_cnt = 0\n",
    "\n",
    "for i in range(0, len(x), batch_size):\n",
    "    x_batch = x[i:i+batch_size]\n",
    "    y_batch = predict(network, x_batch)\n",
    "    p = np.argmax(y_batch,axis = 1) \n",
    "    accuracy_cnt += np.sum(p == t[i:i+batch_size])\n",
    "\n",
    "print(\"Accuracy:\" + str(float(accuracy_cnt) / len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路的學習\n",
    "- 神經網路也就是利用資料，自動決定權重參數的值\n",
    "- 感知收斂定理: 藉由次數有限的學習，解決線性分離問題。但非線性分離問題無法學習\n",
    "- 機器學習的方法(依序為人為、半人為、無人為)\n",
    "   1. 資料 > 人類想出的演算法 > 答案\n",
    "   2. 資料 > 人類想出來的特徵量(SIFT、SURF、HOG等) > 機器學習(SVM、KNN等) > 答案\n",
    "      - 特徵量是指可以從輸入資料(影像)中，精準擷取出本質資料(重要資料)的轉換器\n",
    "      - 需要根據問題使用適當的特徵量，不然也無法獲得良好的結果\n",
    "   3. 資料 > 神經網路(深度學習) > 答案"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失函數\n",
    "- 在神經網路的學習中，會根據某個單一指標，表現目前的狀態，並以該指標為基準找尋**最適合的權重參數**，而該指標就稱為損失函數\n",
    "- 損失函數可以使用任意函數，但主要使用均方誤差或交叉熵誤差\n",
    "- 損失函數小，與訓練資料的誤差較小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 均方誤差(mean squared error): 越小越好\n",
    "- $ E = \\frac {1} {2} \\sum_k (y_k-t_k)^2 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error(y,t):\n",
    "    return 0.5 * np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵誤差(cross entropy error): 越小越好\n",
    "- $ E = - \\sum_k t_k \\log y_k $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7 # 加上微小值 delta 避免 np.log(0) 會變成負無限大的 -inf\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神經網路的梯度法(gradient method)\n",
    "- 了解梯度法前，需先了解中央差分、偏微分及梯度\n",
    "   - 中央差分: 如果用數值微分會有誤差，所以使用中央差分，也就是計算$(x+h)$與$(x-h)$的函數$f$差分，以減少誤差\n",
    "   - 偏微分: 當算式有兩個以上變數，針對其中一個變數進行微分\n",
    "   - 梯度: $(\\frac{\\partial f}{\\partial x_0},\\frac{\\partial f}{\\partial x_1})$ 這種把全部變數的偏微分當作向量來統一的方式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 中央差分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 偏微分範例\n",
    "以 $ f(x_0,x_1) = x^2_0 + x^2_1 $ 為例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 偏微分\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 或 return np.sum(x**2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "- 梯度顯示的方向是在各個位置中，函數值減少最多的方向"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):  # f 為函數，x 為 NumPy 陣列\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # 計算 f(x+h)\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # 計算 f(x-h)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 恢復原值\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "numerical_gradient(function_2, np.array([3.0,4.0])) # 點 (3,4) 的梯度是 (6,8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 梯度法\n",
    "- **用梯度找出函數的最小值的方法**。原理是反覆往梯度方向移動，逐漸減少函數值的方法，就是梯度法\n",
    "- 梯度法是找出梯度為 0 的位置，但卻不見得是最小值，有可能是極小值或是鞍點\n",
    "- 要注意的是，梯度是在各個地點，指出減少最多函數值的方向。無法保證梯度指出的位置使否真的是函數的最小值，以及這個方向是否正確，但朝向該方向，卻能減少最多函數值\n",
    "- 當函數形成複雜的扭曲形狀時，可能進入平坦無法繼續學習的狀態，稱為「停滯期」\n",
    "- 以算式表示:\n",
    "\n",
    "    $ x_0 = x_0 - \\eta \\frac {\\partial f} {\\partial x_0} $\n",
    "\n",
    "    $ x_1 = x_1 - \\eta \\frac {\\partial f} {\\partial x_1} $\n",
    "    - $ \\eta $ 代表更新的量，在神經網路的學習中，稱作**學習率(learning rate)**\n",
    "    - 學習率決定了在一次學習中，要學習多少，更新多少參數\n",
    "    - 學習率太大或太小，都無法達到「適當的位置」\n",
    "- 找出最小值時，會稱為梯度下降法(gradient descent method)；找出最大值時，會稱為梯度上升法(gradient ascent method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 梯度下降法\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100): \n",
    "    # f 是要最佳化的函數，init_x 是預設值，lr 是代表 learning rate 的學習率，step_num 是用梯度法重複的次數\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 梯度法範例(續偏微分範例)\n",
    "用梯度法求出 $ f(x_0,x_1) = x^2_0 + x^2_1 $ 的最小值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0,4.0]) # 假設預設值為(-3.0,4.0)\n",
    "gradient_descent(function_2, init_x = init_x, lr = 0.1, step_num = 100)\n",
    "# 結果為(-6.11110793e-10, 8.14814391e-10)，趨近於(0,0)，實際上最小值也是(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#若學習率太大，會往大數值擴散 (設lr=10.0)\n",
    "init_x = np.array([-3.0,4.0]) # 假設預設值為(-3.0,4.0)\n",
    "gradient_descent(function_2, init_x = init_x, lr = 10.0, step_num = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#若學習率太小，幾乎不會更新就結束 (設lr=1e-10)\n",
    "init_x = np.array([-3.0,4.0]) # 假設預設值為(-3.0,4.0)\n",
    "gradient_descent(function_2, init_x = init_x, lr = 1e-10, step_num = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神經網路的梯度範例\n",
    "假設有個形狀只有2*3、權重為 $W$ 的神經網路，以 $L$ 代表損失函數。此時，可以用 $\\frac {\\partial L} {\\partial W}$ 來表示梯度。算式如下\n",
    "\n",
    "$ W = \n",
    "\\begin{pmatrix}\n",
    "w_{11} & w_{21} & w_{31}\\\\ \n",
    "w_{12} & w_{22} & w_{32}\n",
    "\\end{pmatrix} $\n",
    "\n",
    "$ \\frac {\\partial L}{\\partial W} = \n",
    "\\begin{pmatrix}\n",
    "\\frac {\\partial L}{\\partial w_{11}} & \\frac {\\partial L}{\\partial w_{21}} & \\frac {\\partial L}{\\partial w_{31}}\\\\ \n",
    "\\frac {\\partial L}{\\partial w_{12}} & \\frac {\\partial L}{\\partial w_{22}} & \\frac {\\partial L}{\\partial w_{32}}\n",
    "\\end{pmatrix} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from functions import softmax, cross_entropy_error\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class simpleNet: # 把形狀為2*3的權重參數，當作一個實例變數\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) # 以常態分佈初始化\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W) \n",
    "\n",
    "    def loss(self, x, t): # 計算損失函數用的方法，在引數 x 輸入資料，在 t 輸入正確答案標籤\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18099992  0.38184672 -0.56284664]\n",
      " [ 0.27149988  0.57277008 -0.84426995]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9]) # 函數 f 的引數\n",
    "t = np.array([0, 0, 1]) # 正確答案標籤\n",
    "\n",
    "net = simpleNet()\n",
    "\n",
    "f = lambda w: net.loss(x, t) \n",
    "dW = numerical_gradient(f, net.W) \n",
    "# numerical_gradient(f,x) 是計算梯度，net.W 是權重參數\n",
    "\n",
    "print(dW)\n",
    "# w11 約為 0.2，代表 w11 增加 h ，損失函數的值就會增加 0.2h\n",
    "# w32 約為 -0.5，代表 w32 增加 h ，損失函數減少 0.5h\n",
    "# 從減少損失函數的觀點來看， w32要往正值方向更新， w11 往負值方向更新即可，且 w32 的貢獻比 w11 大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 執行學習演算法\n",
    "- 神經網路的學習步驟:\n",
    "   - 前提: 神經網路擁有適合的權重與偏權值，為了符合訓練資料，而調整權重與偏權值，這就稱作「學習」。神經網路是按照以下四個步驟來學習。\n",
    "   - 步驟一: 小批次\n",
    "      - 從續練資料中，隨機挑選出部分資料\n",
    "      - 挑選出來的資料稱做小批次，這個步驟是以減少小批次的損失函數數值為目標\n",
    "   - 步驟二: 計算梯度\n",
    "      - 為了減少小批次的損失函數，計算出各權重參數的梯度\n",
    "      - 梯度會顯示出損失函數值減少最多的方向\n",
    "   - 步驟三: 更新參數\n",
    "      - 權重參數只往梯度方向進行微量更新\n",
    "   - 步驟四: 重複\n",
    "      - 重複執行步驟一、步驟二、步驟三\n",
    "- 準確率梯度下降(stochastic gradient descent [SGD]): 針對隨機挑選出的資料進行梯度下降法\n",
    "   - 準確率是指「準確隨機挑選」之意"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 範例: 以雙層神經網路為對象，使用MNIST資料集來學習\n",
    "- 權重參數的預設值該如何設定，是決定神經網路學習成功與否的重要關鍵。這裡只簡單介紹權重是按照常態分佈的亂數來進行初始化，偏權值以 0 來初始化，後續會再詳細討論\n",
    "- 為了要測試神經網路是不是能正確辨識訓練資料以外的資料，就要確認有沒有出現「過度學習」的狀況\n",
    "   - 過度學習是指，只有訓練資料內的數字影像才能正確辨識，但是訓練資料以外的數字影像，就無法辨識"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入 TwoLayerNet 類別\n",
    "import sys, os\n",
    "from functions import *\n",
    "from gradient import numerical_gradient\n",
    "\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 權重初始化\n",
    "        self.params = {}  # params 是維持神經網路參數的字典變數(實例變數)\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x): # 進行辨識 # x 是影像資料\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:輸入資料, t:正確答案標籤\n",
    "    def loss(self, x, t): # 計算損失函數\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:輸入資料, t:正確答案標籤\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads # 維持梯度的字典變數\n",
    "        \n",
    "    def gradient(self, x, t): # 誤差反向傳播法，numerical_gradient(self,x,t) 的高速版，後面會深入討論\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        dz1 = np.dot(dy, W2.T)\n",
    "        da1 = sigmoid_grad(a1) * dz1\n",
    "        grads['W1'] = np.dot(x.T, da1)\n",
    "        grads['b1'] = np.sum(da1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 深入了解 TwoLayerNet\n",
    "net = TwoLayerNet(input_size = 784, hidden_size = 100, output_size = 10)\n",
    "net.params['W1'].shape # (784,100)\n",
    "net.params['b1'].shape # (100,)\n",
    "net.params['W2'].shape # (100,10)\n",
    "net.params['b2'].shape # (10,)\n",
    "\n",
    "import numpy as np\n",
    "# np.random.rand(): 生成 [0, 1) 之間的隨機樣本\n",
    "x = np.random.rand(100, 784) # 虛擬輸入資料 (100張)\n",
    "t = np.random.rand(100, 10)  # 虛擬正確答案標籤 (100張)\n",
    "\n",
    "grads = net.numerical_gradient(x, t) # 計算梯度\n",
    "\n",
    "grads['W1'].shape # (784,100)\n",
    "grads['b1'].shape # (100,)\n",
    "grads['W2'].shape # (100,10)\n",
    "grads['b2'].shape # (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc, test acc | 0.11866666666666667, 0.123\n",
      "train acc, test acc | 0.79655, 0.8012\n",
      "train acc, test acc | 0.8795166666666666, 0.8829\n",
      "train acc, test acc | 0.8989, 0.9013\n",
      "train acc, test acc | 0.9072, 0.9105\n",
      "train acc, test acc | 0.9143833333333333, 0.9155\n",
      "train acc, test acc | 0.9191666666666667, 0.9199\n",
      "train acc, test acc | 0.9222166666666667, 0.9233\n",
      "train acc, test acc | 0.92645, 0.9292\n",
      "train acc, test acc | 0.9292333333333334, 0.9319\n",
      "train acc, test acc | 0.9321333333333334, 0.9355\n",
      "train acc, test acc | 0.9348, 0.9362\n",
      "train acc, test acc | 0.93605, 0.9381\n",
      "train acc, test acc | 0.9397333333333333, 0.9414\n",
      "train acc, test acc | 0.9414333333333333, 0.9427\n",
      "train acc, test acc | 0.94385, 0.9445\n",
      "train acc, test acc | 0.9455166666666667, 0.9452\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 使用 MNIST 資料集進行學習\n",
    "# 隨著訓練資料的損失函數值減少，表示神經網路的學習順利進行\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mnist import load_mnist\n",
    "# from two_layer_net import TwoLayerNet\n",
    "\n",
    "# 引入 MNIST 函數\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "# 導入上方 TwoLayerNet 類別\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "# 超參數\n",
    "iters_num = 10000  \n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "#每 1 epoch 的重複次數\n",
    "iter_per_epoch = max(train_size / batch_size, 1)  \n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 取得小批次\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 計算梯度\n",
    "    #grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    grad = network.gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 記錄學習過程\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    #計算 1 epoch 的辨識準確度\n",
    "    if i % iter_per_epoch == 0:   \n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))\n",
    "\n",
    "# 畫圖\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show() #從圖可知，兩種辨識的準確度沒有差別(幾乎重疊)，因此，學習沒有出現過度學習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 誤差反向傳播法\n",
    "- 在計算圖上由左往右進行計算的步驟，稱作正向傳播，其相反則是反向傳播(backward propagation)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
